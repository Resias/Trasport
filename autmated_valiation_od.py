import os
import glob
import numpy as np
import pandas as pd
from tqdm import tqdm
import argparse
import json

# ----------------------------------------------------------
# ğŸ”¥ 1) ëª…ì‹œì ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•˜ëŠ” ì—­ëª… ë§¤í•‘ í…Œì´ë¸”
# ----------------------------------------------------------
SPECIAL_MAP = {
    "4.19ë¯¼ì£¼ë¬˜ì§€": "4Â·19ë¯¼ì£¼ë¬˜ì§€",
    "4Â·19ë¯¼ì£¼ë¬˜ì§€": "4Â·19ë¯¼ì£¼ë¬˜ì§€",
    "ê´€ì•…ì‚°": "ê´€ì•…ì‚°(ì„œìš¸ëŒ€)",
    "ê´€ì•…ì‚°(ì„œìš¸ëŒ€)": "ê´€ì•…ì‚°(ì„œìš¸ëŒ€)",
    "ë‚¨ë™ì¸ë”ìŠ¤íŒŒí¬": "ì¸ë”ìŠ¤íŒŒí¬ë‚¨ë™",
    "ë‹¹ê³ ê°œ": "ë¶ˆì•”ì‚°",
    "ë™ëŒ€ë¬¸ì—­ì‚¬ë¬¸í™”ê³µì›": "ë¬¸í™”ê³µì›ë™ëŒ€ë¬¸ì—­ì‚¬",
    "ëšì„¬ìœ ì›ì§€": "ìì–‘",
    "ì‹œì²­ìš©ì¸ëŒ€": "ì‹œì²­Â·ìš©ì¸ëŒ€",
    "ì‹ ëŒ€ë°©ì‚¼ê±°ë¦¬": "ì‚¼ê±°ë¦¬ì‹ ëŒ€ë°©",
    "ìš´ë™ì¥ì†¡ë‹´ëŒ€": "ìš©ì¸ì¤‘ì•™ì‹œì¥",
    "ì´ìˆ˜": "ì´ì‹ ëŒ€ì…êµ¬",
    "ì¸ì²œêµ­ì œê³µí•­1í„°ë¯¸ë„": "ì¸ì²œê³µí•­1í„°ë¯¸ë„",
    "ì¸ì²œêµ­ì œê³µí•­2í„°ë¯¸ë„": "ì¸ì²œê³µí•­2í„°ë¯¸ë„",
    "ì „ëŒ€ì—ë²„ëœë“œ": "ì „ëŒ€Â·ì—ë²„ëœë“œ",
    "ì§€ì œ": "í‰íƒì§€ì œ",
    "í™”ì „": "í•œêµ­í•­ê³µëŒ€",
    "í‘ì„(ì¤‘ì•™ëŒ€ì…êµ¬)": "í‘ì„",
    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: ì²­ëŸ‰ë¦¬(ì§€ìƒ), ì²­ëŸ‰ë¦¬(ì§€í•˜)
    "ì²­ëŸ‰ë¦¬(ì§€ìƒ)": "ì²­ëŸ‰ë¦¬",
    "ì²­ëŸ‰ë¦¬(ì§€í•˜)": "ì²­ëŸ‰ë¦¬",
    # ì•„ì‹œì•„ë“œê²½ê¸°ì¥
    "ì•„ì‹œì•„ë“œê²½ê¸°ì¥": "ì•„ì‹œì•„ë“œê²½ê¸°ì¥",
    "ì•„ì‹œì•„ë“œê²½ê¸°ì¥(ê³µì´Œì‚¬ê±°ë¦¬": "ì•„ì‹œì•„ë“œê²½ê¸°ì¥",
    "ì•„ì‹œì•„ë“œê²½ê¸°ì¥(ê³µì´Œì‚¬ê±°ë¦¬)": "ì•„ì‹œì•„ë“œê²½ê¸°ì¥"
}

import re
def remove_parentheses(name: str):
    return re.sub(r"\(.*?\)", "", name).strip()

def normalize_station_name(name: str):
    if not isinstance(name, str):
        return "Unknown"

    name = name.strip()
    if name in SPECIAL_MAP:
        return SPECIAL_MAP[name]

    return remove_parentheses(name)


# =====================================================
# 1) Parquet ê¸°ë³¸ êµ¬ì¡° ê²€ì‚¬
# =====================================================
def validate_parquet_structure(parquet_files):
    print("\n==============================")
    print("ğŸ“Œ [1] Parquet êµ¬ì¡° ê²€ì¦")
    print("==============================")

    required_cols = ["ìŠ¹ì°¨ì—­ëª…", "í•˜ì°¨ì—­ëª…", "ìŠ¹ì°¨ì¼ì‹œ"]

    sample = pd.read_parquet(parquet_files[0])
    print("ìƒ˜í”Œ íŒŒì¼:", parquet_files[0])

    missing = [c for c in required_cols if c not in sample.columns]
    if missing:
        print("âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½:", missing)
    else:
        print("âœ” í•„ìˆ˜ ì»¬ëŸ¼ OK")

    print("\nNaN ë¹„ìœ¨ ì ê²€:")
    for col in required_cols:
        print(f"  - {col}: {sample[col].isna().mean():.4f}")

    t = pd.to_datetime(sample["ìŠ¹ì°¨ì¼ì‹œ"], errors="coerce")
    print(f"\nâœ” ì‹œê°„ íŒŒì‹± ì„±ê³µë¥ : {(~t.isna()).mean():.4f}")

    print("\nìƒ˜í”Œ 5ê°œ:")
    print(sample.head())
    print()


# =====================================================
# 2) station2id ì‚¬ì „ ê²€ì¦
# =====================================================
def validate_station_dict(station2id):
    print("\n==============================")
    print("ğŸ“Œ [2] ì—­ ì‚¬ì „(station2id) ê²€ì¦")
    print("==============================")

    print("ì´ ì—­ ê°œìˆ˜:", len(station2id))
    print("Unknown ê°œìˆ˜:", sum(1 for s in station2id if s == "Unknown"))

    if len(station2id) == len(set(station2id.keys())):
        print("âœ” ì¤‘ë³µ ì—†ìŒ")
    else:
        print("âŒ ì¤‘ë³µ ìˆìŒ")


# =====================================================
# 3) OD ë§¤íŠ¸ë¦­ìŠ¤ ê²€ì¦
# =====================================================
def validate_minute_od(od_path, parquet_files, station2id):

    print("\n==============================")
    print("ğŸ“Œ [3] OD ë§¤íŠ¸ë¦­ìŠ¤ ê²€ì¦")
    print("==============================")

    OD = np.load(od_path)
    print("OD shape:", OD.shape)

    minute_slots, N1, N2 = OD.shape
    N = len(station2id)

    print("âœ” 1440ë¶„ OK" if minute_slots == 1440 else "âŒ minute ì˜¤ë¥˜")
    print("âœ” ì—­ ê°œìˆ˜ OK" if (N1 == N and N2 == N) else f"âŒ ì—­ ê°œìˆ˜ ë¶ˆì¼ì¹˜: OD={N1}, dict={N}")

    # Unknown ì œì™¸í•œ ì „ì²´ ìŠ¹ì°¨ ìˆ˜ ê³„ì‚°
    total_rides = 0
    for f in parquet_files:
        df = pd.read_parquet(f, columns=["ìŠ¹ì°¨ì—­ëª…", "í•˜ì°¨ì—­ëª…"])
        df["ìŠ¹ì°¨ì—­ëª…"] = df["ìŠ¹ì°¨ì—­ëª…"].apply(normalize_station_name)
        df["í•˜ì°¨ì—­ëª…"] = df["í•˜ì°¨ì—­ëª…"].apply(normalize_station_name)
        df = df[(df["ìŠ¹ì°¨ì—­ëª…"] != "Unknown") & (df["í•˜ì°¨ì—­ëª…"] != "Unknown")]
        total_rides += len(df)

    od_sum = OD.sum()

    print(f"\nì´ ìŠ¹ì°¨ ìˆ˜(Unknown ì œì™¸): {total_rides:,}")
    print(f"OD ì´í•©:                 {od_sum:,}")

    if abs(total_rides - od_sum) <= max(1, total_rides * 0.001):
        print("âœ” ì´í•© ì¼ì¹˜")
    else:
        print("âŒ ì´í•© ë¶ˆì¼ì¹˜ â€” íŒŒì‹± ê·œì¹™ ì°¨ì´ ê°€ëŠ¥")

    # ë¶„ë‹¹ ë¶„í¬ ì˜ˆì‹œ
    print("\nì‹œê°„ëŒ€ë³„ ë¶„í¬(0~10ë¶„):")
    print(OD.sum(axis=(1,2))[:10])
    print()


# =====================================================
# 4) station2idì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ì—­ëª… í™•ì¸
# =====================================================
def check_missing_station_names(parquet_files, station2id):

    print("\n==============================")
    print("ğŸ“Œ [4] station2idì— ì—†ëŠ” ì—­ëª… ê²€ì‚¬")
    print("==============================")

    valid_stations = set(station2id.keys())
    missing = set()

    for f in tqdm(parquet_files, desc="ì—­ëª… ê²€ì‚¬"):
        df = pd.read_parquet(f, columns=["ìŠ¹ì°¨ì—­ëª…","í•˜ì°¨ì—­ëª…"])
        df["ìŠ¹ì°¨ì—­ëª…_norm"] = df["ìŠ¹ì°¨ì—­ëª…"].apply(normalize_station_name)
        df["í•˜ì°¨ì—­ëª…_norm"] = df["í•˜ì°¨ì—­ëª…"].apply(normalize_station_name)

        missing.update(set(df["ìŠ¹ì°¨ì—­ëª…_norm"]) - valid_stations)
        missing.update(set(df["í•˜ì°¨ì—­ëª…_norm"]) - valid_stations)

    missing.discard("Unknown")

    print("\nğŸ”¥ station2idì— ì—†ëŠ” ì—­ëª… ëª©ë¡:")
    for n in sorted(missing):
        print(" -", n)

    print(f"\nì´ {len(missing)}ê°œì˜ ì—­ëª…ì´ station2idì— ì—†ìŒ\n")


# =====================================================
# Main
# =====================================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", default="./202205/train_pars")
    parser.add_argument("--od_dir", default="./od_minute")
    parser.add_argument("--save_json", default="./station2id.json")
    args = parser.parse_args()

    parquet_files = sorted(glob.glob(os.path.join(args.data, "*.parquet")))
    if not parquet_files:
        print("âŒ parquet ì—†ìŒ")
        return

    # 1) parquet êµ¬ì¡° ê²€ì¦
    validate_parquet_structure(parquet_files)

    # 2) station2id ìƒì„± ë° ê²€ì¦
    from od_making import build_global_station_dict
    station2id = build_global_station_dict(parquet_files)

    validate_station_dict(station2id)

    # ì €ì¥
    with open(args.save_json, "w", encoding="utf-8") as f:
        json.dump(station2id, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ station2id ì €ì¥ ì™„ë£Œ â†’ {args.save_json}")

    # 3) OD ê²€ì¦
    od_files = sorted(glob.glob(os.path.join(args.od_dir, "OD_minute_*.npy")))
    if not od_files:
        print("âŒ OD íŒŒì¼ ì—†ìŒ")
        return

    import random
    idx = random.randrange(0,len(od_files))
    sample_od = od_files[idx]
    date = os.path.basename(sample_od).split("_")[2].split(".")[0]
    day_files = [f for f in parquet_files if date in f]

    validate_minute_od(sample_od, day_files, station2id)

    # 4) station2idì— ì—†ëŠ” ì—­ëª… ê²€ì‚¬
    check_missing_station_names(parquet_files, station2id)

    print("\n==============================")
    print("ğŸ‰ ì „ì²´ ê²€ì¦ ì™„ë£Œ")
    print("==============================\n")


if __name__ == "__main__":
    main()
